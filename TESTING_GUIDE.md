# AI Manager Testing Guide

This guide explains how to test the AI Manager with sample inputs and see outputs on screen.

## ğŸš€ Quick Start

### 1. Start the Server
```bash
# Activate virtual environment
source venv/bin/activate

# Start the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Test the AI Agent

#### **Option A: Automated Test (Recommended)**
```bash
python test_ai_agent.py
```
This script will:
- âœ… Login automatically
- âœ… Create a demo project with tags
- âœ… Create multiple runs with different configurations
- âœ… Simulate training with realistic metrics
- âœ… Show all results on screen

#### **Option B: Interactive Test**
```bash
python interactive_test.py
```
This provides an interactive menu where you can:
- ğŸ“ Create your own projects
- ğŸƒ Create your own runs
- ğŸ“Š Log custom metrics
- ğŸ¯ Simulate training with your parameters

#### **Option C: Enhance Existing Demo**
```bash
python enhance_demo.py
```
This will add tags to your existing demo projects and create additional demo runs.

## ğŸ¯ What You'll See

### **Automated Test Output:**
```
ğŸ¤– AI Manager Test Agent
==================================================

1ï¸âƒ£ Testing Login...
âœ… Logged in successfully as admin

2ï¸âƒ£ Creating Demo Project with Tags...
âœ… Created project: demo_image_classifier (ID: 3)
   Tags: demo, image-classification, cnn, pytorch, computer-vision

3ï¸âƒ£ Creating Test Runs with Tags...
âœ… Created run: baseline_cnn (ID: 25)
   Tags: baseline, cnn, small-model
âœ… Created run: large_resnet (ID: 26)
   Tags: large-model, resnet, high-accuracy
âœ… Created run: experiment_attention (ID: 27)
   Tags: experiment, attention, research

4ï¸âƒ£ Simulating Training...
ğŸ¯ Simulating training for run 25...
âœ… Epoch 0: loss=1.9000, acc=0.3000
âœ… Epoch 1: loss=1.7100, acc=0.3600
âœ… Epoch 2: loss=1.5390, acc=0.4200
âœ… Training simulation completed for run 25

5ï¸âƒ£ Listing All Projects and Runs...
ğŸ“ Found 2 projects:
   â€¢ demo_image_classifier (ID: 3) [demo, image-classification, cnn, pytorch, computer-vision]
   â€¢ sample_image_classifier (ID: 4) [demo, image-classification, cnn, pytorch, computer-vision]

ğŸƒ Found 3 runs for project 3:
   â€¢ baseline_cnn (ID: 25) - completed [baseline, cnn, small-model]
   â€¢ large_resnet (ID: 26) - completed [large-model, resnet, high-accuracy]
   â€¢ experiment_attention (ID: 27) - completed [experiment, attention, research]

âœ… Test completed! Check the web dashboard at http://localhost:8000
```

### **Interactive Test Menu:**
```
ğŸ¯ Interactive AI Manager Test Menu
==================================================
1. List all projects
2. Create new project
3. List runs for a project
4. Create new run
5. Log metrics manually
6. Simulate training
7. Exit
--------------------------------------------------
Choose an option (1-7): 2

ğŸ“ Create New Project
------------------------------
Project name: my_test_project
Description (optional): Testing image classification
Enter tags (comma-separated, e.g., demo,cnn,pytorch): test,cnn,experiment
Tags: test, cnn, experiment
âœ… Created project: my_test_project (ID: 5)
   Tags: test, cnn, experiment
```

## ğŸ·ï¸ Tag System

### **Project Tags:**
- `demo` - Demo projects
- `image-classification` - Computer vision projects
- `nlp` - Natural language processing
- `computer-vision` - Image/video processing
- `pytorch` - PyTorch framework
- `tensorflow` - TensorFlow framework
- `research` - Research experiments
- `production` - Production models

### **Run Tags:**
- `baseline` - Baseline models
- `experiment` - Experimental runs
- `small-model` - Lightweight models
- `large-model` - Complex models
- `high-accuracy` - High-performance runs
- `fast-training` - Quick training runs
- `attention` - Attention mechanisms
- `resnet` - ResNet architectures

## ğŸ“Š Testing Metrics

### **Manual Metric Logging:**
```
ğŸ“Š Log Metrics for Run 25
-----------------------------------
Enter metrics (or 'done' to finish):
Format: metric_name=value (e.g., loss=0.5, accuracy=0.85)
Metrics: loss=0.45, accuracy=0.92, f1_score=0.89
Step (default: 0): 10
âœ… Logged metrics for step 10: {'loss': 0.45, 'accuracy': 0.92, 'f1_score': 0.89}
```

### **Simulated Training:**
```
ğŸ¯ Simulate Training for Run 25
----------------------------------------
Number of epochs (default: 10): 5
Delay between epochs in seconds (default: 0.5): 0.3

ğŸ¯ Simulating 5 epochs with 0.3s delay...
âœ… Epoch 0: loss=1.9000, acc=0.3000
âœ… Epoch 1: loss=1.7100, acc=0.3600
âœ… Epoch 2: loss=1.5390, acc=0.4200
âœ… Epoch 3: loss=1.3851, acc=0.4800
âœ… Epoch 4: loss=1.2466, acc=0.5400
âœ… Training simulation completed for run 25
```

## ğŸŒ Web Dashboard

After running the tests, visit **http://localhost:8000** to see:

### **Projects with Tags:**
- ğŸ“ **demo_image_classifier** `[demo, image-classification, cnn, pytorch, computer-vision]`
- ğŸ“ **sample_image_classifier** `[demo, image-classification, cnn, pytorch, computer-vision]`

### **Runs with Tags:**
- ğŸƒ **baseline_cnn** `[baseline, cnn, small-model]` - completed
- ğŸƒ **large_resnet** `[large-model, resnet, high-accuracy]` - completed
- ğŸƒ **experiment_attention** `[experiment, attention, research]` - completed

### **Real-time Metrics:**
- ğŸ“Š Training curves updating live
- ğŸ“ˆ Loss and accuracy plots
- ğŸ”„ WebSocket updates

## ğŸ”§ Custom Testing

### **Create Your Own Test:**
```python
from test_ai_agent import AIManagerTestAgent

# Initialize agent
agent = AIManagerTestAgent()

# Login
agent.login()

# Create custom project
project = agent.create_test_project(
    name="my_custom_project",
    description="My custom AI project",
    tags=["custom", "neural-network", "research"]
)

# Create custom run
run = agent.create_test_run(
    project_id=project['id'],
    name="my_experiment",
    tags=["experiment", "attention", "transformer"]
)

# Simulate training
agent.simulate_training(run['id'], epochs=20)
```

### **Test Different Scenarios:**
1. **Image Classification**: Use tags like `cnn`, `resnet`, `computer-vision`
2. **NLP Projects**: Use tags like `nlp`, `transformer`, `bert`
3. **Research**: Use tags like `research`, `experiment`, `attention`
4. **Production**: Use tags like `production`, `optimized`, `deployed`

## ğŸ› Troubleshooting

### **Common Issues:**

1. **Connection Error:**
   ```
   âŒ Connection error: Connection refused
   ```
   **Solution:** Make sure the server is running on port 8000

2. **Login Failed:**
   ```
   âŒ Login failed: 401
   ```
   **Solution:** Use username `admin` and password `admin123`

3. **Project Creation Failed:**
   ```
   âŒ Failed to create project: 400
   ```
   **Solution:** Project name already exists, try a different name

4. **Metrics Not Updating:**
   ```
   âŒ Failed to log metrics: 404
   ```
   **Solution:** Make sure the run ID exists and is valid

### **Debug Mode:**
Add debug output to see detailed API calls:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ¯ Next Steps

1. **Explore the Web Dashboard** - See your projects and runs with tags
2. **Try Interactive Testing** - Create your own custom experiments
3. **Add Real Training** - Integrate with your actual ML training code
4. **Custom Metrics** - Log your own custom metrics and visualizations

Happy testing! ğŸš€ 